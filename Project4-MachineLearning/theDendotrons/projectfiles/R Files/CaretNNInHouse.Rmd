---
title: "XGBoost"
author: "Jason Sippie"
date: "November 19, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(caret)
library(dtplyr)
library(data.table)
library(Matrix)
library(Metrics)
library(doMC)
library(doParallel)


```

## Preprocessing

```{r cars}
cl <- makeCluster(8)
registerDoParallel(cl)


# read in data and filter out cat80  = D
t.train = read.csv("train.csv", stringsAsFactors = F)


# varsToKeep = c("cat80", "cont7", "cat57", "cat79", "cont2", "cat12", "cat81", "cont12", "cont14","cat87", "cat72", "cat100", "cat1", "cat10", "cat103","cat114", "cat111","cont3","cat113","cont11", "loss")

#varsToKeep = c("cat80","cont2","cat12","cat79","cont7","cont14","cat81","cat57","cat1","isSmall")
#t.train = t.train[,varsToKeep]

# Reduce the number of columns based on sparsity
nzv <- nearZeroVar(t.train)
t.train = t.train[,-nzv]


TARGET = 'loss'

t.train$id = NULL

#to shrink down the initial set
# set.seed(313)
# index.sub = createDataPartition(t.train$cat80, p=0.3,list = FALSE)
# t.train = t.train[index.sub,]


set.seed(313)
index.sub = createDataPartition(t.train$cat80, p=0.8,list = FALSE)

train <- t.train[index.sub,]
test <- t.train[-index.sub,]


ntrain = nrow(train)
train_test = rbind(train, test)

# turn characters into integers
features = names(train)
for (f in features) {
  if (class(train_test[[f]])=="character") {
    levels <- unique(train_test[[f]])
    train_test[[f]] <- as.integer(factor(train_test[[f]], levels=levels))
  }
}

# scale/center

preProcValues <- preProcess(train_test, method = c("center", "scale"))
train_testTrans <- predict(preProcValues, train_test)

y_train = log(train[,TARGET])
y_test = log(test[,TARGET])



x_train = train_testTrans[1:ntrain,]
x_test = train_testTrans[(ntrain+1):nrow(train_test),]

# one hot encoding
dm_train <- model.matrix(loss ~ ., data = x_train)
dm_test <- model.matrix(loss ~ ., data = x_test)

dm_train <- dm_train[,-1]
dm_test <- dm_test[,-1]


x_mae <-function (data, lev = NULL, model = NULL,...) 
{ 
  require(Metrics)
  m <- try(Metrics::mae(exp(data$obs), exp(data$pred)),silent=TRUE)
  out<-c(m)
  names(out) <- c("MAE")
  out 
}

nn_trcontrol <- trainControl(
  method="cv",
  number = 2,
  verboseIter = TRUE,
  returnData=FALSE,
  returnResamp = "all",
  allowParallel = TRUE,
summaryFunction = x_mae
)

## for nnet package
tunegrid <- expand.grid(.decay = 0.05, .size = c(4))

nn_train_1 <- train(
  x = dm_train,
  y= y_train,
 trControl = nn_trcontrol,
 tuneGrid = tunegrid,
 method="nnet",
 verbose=1,
 maxit=2000,
 linout=T,
  maximize=F,
  metric="MAE")


save(nn_train_1, file="nn_train_1_model_1126")


train.predict = predict(nn_train_1, dm_train)
trainRes = data.frame(train.predict, y_train)
trainRes$absDiff = abs(exp((trainRes[,1]))-(exp(trainRes[,2])))
mean(trainRes$absDiff)

plot(trainRes[,1], trainRes[,2])


test.predict = predict(nn_train_1, dm_test)
testRes = data.frame(pred=exp(test.predict), obs=exp(y_test))
testRes$diff = testRes$pred - testRes$obs
testRes$absDiff = abs(testRes$diff)
testRes$absLogDiff = log(testRes$absDiff)*sign(testRes$diff)
mean(testRes$absDiff)

plot(log(testRes$pred), testRes$diff)

plot(log(testRes$obs), testRes$absLogDiff)

testResNN$Model = "Nnet"

XGBTestRes$Model = "XGB"

r = rbind(XGBTestRes, testResNN)




```