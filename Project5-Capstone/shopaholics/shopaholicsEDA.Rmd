---
title: "shopaholicsEDA"
author: "Jhonasttan Regalado"
date: "12/3/2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/DataScience/bootcamp7/githubcrush/bootcamp007_project/Project5-Capstone/shopaholics/BX-CSV-Dump/")
```

Load libraries
```{r Load Libraries, echo=FALSE}
library(tidyverse)
library(ggmap)
library(ggplot2)
library(anytime)
library(lubridate)
library(leaflet)
library(jsonlite)
library(caret)
library(stringr)
library(plotly)
library(VIM)
library(mice)

```

Load user data into dataframe and capture geospatial locations
```{r Load users}
users <- read.csv("./BX-Users.csv", sep = ";", header = TRUE, stringsAsFactor=FALSE, quote="\"", encoding = "UTF-8") 
str(users)

```

Cleanup users data
```{r users data cleanup}
#remove bad values for integer(Age), integer(UserID) and character(Location)

for (i in seq_along(users$Age)) {
    users$Age[i] <- tryCatch(as.integer(users$Age[i]),
                                error=function(e) NA)
    
    users$User.ID[i] <- tryCatch(as.integer(users$User.ID[i]),
                                error=function(e) NA)
    
     users$Location[i] <- tryCatch(as.character(users$Location[i]),
                                error=function(e) NA)

}

#remove '.' in column names and filter out NAs on UserID
colnames(users) <- c("UserID", "Location", "Age")

for (i in seq_along(users$Location)) {
  loc <- str_split(users$Location[i], ",")[[1]]
  users$city[i] <- gsub(" ", "", loc[1])
  users$state[i] <- gsub(" ", "", loc[2])
  users$country[i] <- gsub(" ", "", loc[3])
}

#write.csv(filter(users, !is.na(UserID)), "./usersCleaned.csv",row.names = FALSE)
write.csv(users, "./usersCleanedWithNAs.csv",row.names = FALSE)

#scrubber >> users$Age <- str_replace_all(users$Age, "&#", "")
#wasn't required  >> users$Age <- str_replace_all(users$Age, "<f3>dzk<69>e", "")

```

Code to get geoCodes

```{r loc get geocodes}

locations <- users %>% group_by(Location) %>% count() %>% arrange(desc(n))

userLocationGeoSpatial <- function(x) {
  address <- geocode(location = x)
}

#get address geolocations
#for (i in seq_along(locations$Location)) {
for (i in seq(1501,1700)) {
  locationGeocode <- c(0.0,0.0)
  
  locationGeocode <- tryCatch(
    userLocationGeoSpatial(locations$Location[i]), 
    error=function(e) NULL
    )
  
  if (length(locationGeocode[1]) != 0) {
  
    locations$lon[i] <- locationGeocode[[1]]
    locations$lat[i] <- locationGeocode[[2]]

  }
  
  Sys.sleep(1)
}

write.csv(locations[1:1700,], file = './locations.csv')
```

Load locations with Geocodes
```{r loc - load w geocodes}
locationsGeoCoded <- read.csv("./locations.csv")

```


View users by age groups -- User data needs cleaning (python??) prior to grouping

```{r}

users %>% filter(Age > 0) %>% ggplot(aes(Age)) + geom_bar()
```

Load books
```{r load books - clean}
books <- read.csv("./BX-Books.csv",sep = ";", header = TRUE, stringsAsFactor=FALSE, quote="\"")
#books <- read.csv("./BX-Books-munged.csv",sep = ";", header = TRUE)

#clean books data set
for (i in seq_along(books$ISBN)) {
    books$ISBN[i] <- tryCatch(as.integer(books$ISBN[i]),
                                error=function(e) NA)
    books$Book.Title[i] <- tryCatch(as.character(books$Book.Title[i]),
                                error=function(e) NA)
    books$Book.Author[i] <- tryCatch(as.character(books$Book.Author[i]),
                                error=function(e) NA)
    books$Publisher[i] <- tryCatch(as.character(books$Publisher[i]),
                                error=function(e) NA)
}

books$Year.Of.Publication <-  lubridate::year(anydate(books$Year.Of.Publication))

colnames(books) <- c("ISBN", "BookTitle", "BookAuthor", "YearOfPublication", "Publisher",
                     "ImageUrlSmall", "ImageUrlMedium", "ImageUrlLarge")
write.csv(books, "./booksCleanedWithNAs.csv",row.names = FALSE)


```

Load ratings
```{r}
ratings <- read.csv("./BX-Book-Ratings.csv", sep = ";", header = TRUE, stringsAsFactor=FALSE)

for (i in seq_along(ratings$User.ID)) {
    ratings$User.ID[i] <- tryCatch(as.integer(ratings$User.ID[i]),
                                error=function(e) NA)
    ratings$Book.Rating[i] <- tryCatch(as.integer(ratings$Book.Rating[i]),
                                error=function(e) NA)
}

colnames(ratings) <- c("UserID", "ISBN", "BookRating")
write.csv(ratings, "./ratingsCleanedWithNAs.csv",row.names = FALSE)

# 35% NA >> 176019 / 493813 = 0.3564487
# length(is.na(ratings$BookRating))
# [1] 493813
# > length(ratings$BookRating[ratings$BookRating > 0])
# [1] 176019
# > dim(ratings[ratings$BookRating > 0,])
# [1] 176019      3

ratingsWoutZeroes  <- ratings[ratings$BookRating > 0, ]
write.csv(ratingsWoutZeroes, "./ratingsCleanedWithoutZeroes.csv",row.names = FALSE)
```

EDA

Missingness analysis
```{r Missingness}
aggr(users)

aggr(users, plot=FALSE)

 # Missings in variables:
 # Variable Count
 #   UserID    79
 #      Age 55697
 #    state    48
 #  country    55

aggr(ratings)

aggr(books)
aggr(books, plot = FALSE)

# Missings in variables:
#           Variable Count
#               ISBN 19680
#  YearOfPublication  3549

#md.pattern()

```


Summary:
```{r books summary}
summary(books)
str(books)
#books$Year.Of.Publication <- as.Date(books$Year.Of.Publication, "%Y")
head(books)
summary(users)
head(users)
summary(ratings)
str(books)
```
Books - 113,679 unique ISBNs without category / subject or country of origin
```{r}
summary(books$Year.Of.Publication)
ggplot(books,aes(Year.Of.Publication)) + geom_histogram()
filter(books,Year.Of.Publication == 2050)
filter(books,Year.Of.Publication != 2050) %>% ggplot(.,aes(Year.Of.Publication)) + geom_histogram()
books %>% group_by(Year.Of.Publication) %>% count() %>% top_n(.,10) %>% arrange(desc(n))
```

Users


Leafmap for visualizing locations - data was also ported to Carto >> https://jhonasttan.carto.com/builder/d2dc0256-ba5f-11e6-bfdd-0e8c56e2ffdb/embed?state=%7B%22map%22%3A%7B%22ne%22%3A%5B-75.14077784070427%2C-211.28906249999997%5D%2C%22sw%22%3A%5B82.35580019800932%2C203.203125%5D%7D%2C%22widgets%22%3A%7B%2225e9dd14-0976-4ce4-b198-6b22eca59f52%22%3A%7B%22normalized%22%3Atrue%7D%7D%7D 
```{r leaflet}
#set leaflet map tile
tile_layer <- "https://api.mapbox.com/styles/v1/mapbox/streets-v10/tiles/256/{z}/{x}/{y}?access_token=pk.eyJ1IjoiamhvbmFzdHRhbiIsImEiOiJFLTAzeVVZIn0.mwAAfKtGwv3rs3L61jz87A"

# leaflet(data = users) %>% 
#   addTiles(urlTemplate = tile_layer) %>% 
#   addMarkers(popup = paste0("Station: ", leaflet_info$stationName,
#                             "<br>Bikes: ", leaflet_info$availableBikes, " / Docks: ",
#                             leaflet_info$availableDocks, " / Total Docks: ", leaflet_info$totalDocks)) %>%
#   addCircles(lat = leaflet_info[1:length(input$selected),'latitude'], 
#              lng = leaflet_info[1:length(input$selected),'longitude'],
#              weight = leaflet_info$availableBikes, radius = leaflet_info$availableDocks / input$docksAvailable, 
#              color =  ifelse(leaflet_info$availableBikes >= input$bikesAvailable,"green","red"),
#              #weight = 1, radius = ((leaflet_info$availableBikes + leaflet_info$availableDocks) * 5 ), color =  "black",
#              fillColor = "orange", fillOpacity=0.5, opacity=1) %>%
# setView(lng = -73.976522, lat = 40.7528, zoom = input$zoom)

leaflet(data = locations[1:75,c("Location","lon","lat")]) %>% 
  addTiles(urlTemplate = tile_layer) %>% 
  addMarkers(popup = locations$Location) # %>%
  #setView(lng = -73.976522, lat = 40.7528, zoom = 12)
```

Ratings - how do we handle zero ratings?

```{r ggplot}
ggplot(ratings, aes(Book.Rating)) + geom_histogram()
```

Capture Book's current rating and other features from Google API (key not required) - work in progress
Example: https://www.googleapis.com/books/v1/volumes?q=isbn:0195153448

```{r GoogleAPI}

#testing
#bookUrlStart <- "https://www.googleapis.com/books/v1/volumes?q=isbn:"
#isbn <- books$ISBN[1]
#bookUrl <- paste0("https://www.googleapis.com/books/v1/volumes?q=isbn:", isbn)
#bookJson <- fromJSON(paste(readLines(bookUrl), collapse=""))

#booksCatalogue <- data.frame()
#googleBooksCat <- list()

#Build book catalogue
#isbn
jasonIsbnParse <- function(bookJson){
  title <- bookJson$items$volumeInfo$title
  authors <- paste(unlist(bookJson$items$volumeInfo$authors), collapse = ", ") #can be more than one 
  publishedDate <- bookJson$items$volumeInfo$publishedDate
  publisher <- bookJson$items$volumeInfo$publisher
  categories <- paste(unlist(bookJson$items$volumeInfo$categories), collapse = ", ")
  pageCount <- bookJson$items$volumeInfo$pageCount
  averageRating <- bookJson$items$volumeInfo$averageRating
  ratingsCount <- bookJson$items$volumeInfo$ratingsCount
  maturityRating <- bookJson$items$volumeInfo$maturityRating
  imageThumbnail <- bookJson$items$volumeInfo$imageLinks$smallThumbnail
  infoLink <- bookJson$items$volumeInfo$infoLink
  textSnippet <- bookJson$items$searchInfo$textSnippet
  
  #create list for adding to dataframe
  googleBooksCat <- list(isbn = isbn, title = title, authors = authors, 
                         publishedDate = publishedDate, publisher = publisher, 
                         categories = categories, pageCount = pageCount, 
                         averageRating = averageRating, ratingsCount = ratingsCount,
                         maturityRating = maturityRating, imageThumbnail = imageThumbnail, 
                         infoLink = infoLink, textSnippet = textSnippet)
  as.data.frame(googleBooksCat)
}
#add row to dataframe
#as.data.frame(googleBooksCat)
#booksCatalogue <- rbind(booksCatalogue, as.data.frame(googleBooksCat))

isbn1k <- read.csv("./books1000.csv",header = TRUE, stringsAsFactors = FALSE)

tryCatch.W.E <- function(expr) {
    W <- NULL
    w.handler <- function(w){ # warning handler
	W <<- w
	invokeRestart("muffleWarning")
    }
     list(value = withCallingHandlers(tryCatch(expr, error = function(e) e),
 				     warning = w.handler),
 	 warning = W)
}

booksCatalogue <- data.frame()
googleBooksCat <- list()
bookJsonWE <- list()

# for (i in seq_along(unlist(isbn1k))) {
#     isbn <- isbn1k[[1]][i]
#   bookUrl <- paste0("https://www.googleapis.com/books/v1/volumes?q=isbn:", isbn)
#   
#   bookJson <- list()
#   bookJsonWE[i] <- tryCatch.W.E(bookJson <- fromJSON(paste(readLines(bookUrl), collapse="")))
#   
#   if (length(bookJson) > 0) {
#     booksCatalogue <- rbind(booksCatalogue, jasonIsbnParse(bookJson))
#   }
#   Sys.sleep(1/2)
# }

for (i in seq_along(unlist(isbn1k))) {
#for (i in c(67:1000)) {
    isbn <- isbn1k[[1]][i]
  bookUrl <- paste0("https://www.googleapis.com/books/v1/volumes?q=isbn:", isbn, "&key=AIzaSyA4QM4037gM47V23b7vb-6WHAceBzkOgv4")
  
  bookJson <- list()
  bookJson <- fromJSON(paste(readLines(bookUrl), collapse=""))
  
  if (length(bookJson) > 2) {
    bookJsonWE[i] <- tryCatch.W.E( booksCatalogue <- rbind(booksCatalogue, jasonIsbnParse(bookJson)) )
  }
  Sys.sleep(1/2)
}
#convert the column publishedDates to dates
booksCatalogue$publishedDate <- anydate(booksCatalogue$publishedDate)
```

EDA on google books review
```{r google EDA}

#Dates histogram
ggplot(booksCatalogue, aes(publishedDate)) + geom_histogram()
summary(booksCatalogue$publishedDate)

#Avg rating histogram
ggplot(booksCatalogue, aes(averageRating)) %>% + geom_histogram()
ggplot(booksCatalogue, aes(ratingsCount, averageRating)) %>% + geom_point()

#Avg rating vs Rating count
ggplot(booksCatalogue, aes(ratingsCount, averageRating)) + geom_smooth(se = FALSE)

summary(booksCatalogue$averageRating)
summary(booksCatalogue$ratingsCount)

#Avg rating vs page count
ggplot(booksCatalogue, aes(ratingsCount, pageCount)) + geom_smooth(se = FALSE)
ggplot(booksCatalogue, aes(ratingsCount, pageCount)) + geom_point()
p1 <- ggplot(booksCatalogue, aes(pageCount, ratingsCount)) + geom_point(aes(color = categories)) + theme(legend.position = "top")
ggplotly(p1)

# p1 + guides(fill=guide_legend(nrow=1,byrow=TRUE))
# 
# library(stringr)
# labels = function(x) str_wrap(x, width = 10)
# #scale_color_manual(labels = c("T999", "T888"), values = c("blue", "red"))
# p1 + scale_color_manual(labels = function(categories) str_wrap(categories, width = 10), values = colors()[1:length(x)])
# 
# booksCatalogue$labels = substr(booksCatalogue$categories, 1, 10)
# 
# booksCatalogue$labels = str_wrap(booksCatalogue$categories, width = 10)
# Categories = factor(booksCatalogue$labels)
# l = levels(Categories)
# 
# 
# plot(mydf$Date, mydf$Avg, xlab="Date", ylab="Avg", col=f)
# legend("topright", legend = l, fill = 1:length(l), title = "Prods")

booksCatalogue$labels = str_wrap(booksCatalogue$categories, width = 10)
Categories = factor(booksCatalogue$labels)
l = levels(Categories)

ggplot(booksCatalogue, aes(pageCount, ratingsCount)) + geom_point(aes(color = Categories)) #+ theme(legend.position = "top")

ggplot(booksCatalogue, aes(pageCount, averageRating)) + geom_point(aes(color = Categories)) #+ guides(fill=guide_legend(nrow=1,byrow=TRUE,legend.position="top"))

ggplot(booksCatalogue, aes(averageRating, ratingsCount)) + geom_point(aes(color = Categories))

#what are genres and page counts for highly rated books?
```


Predict Ratings Model
```{r Modeling}
set.seed(0)
#divide data into half via sampling to test prediction models
subsetRatingsIndex <- createDataPartition(ratings$User.ID, list=FALSE)
subsetRatings <- ratings[subsetRatingsIndex, ]

tIndex <- createDataPartition(subsetRatings$User.ID, list = FALSE, p=0.7)

train_ratings <- subsetRatings[tIndex,]
test_ratings <- subsetRatings[-tIndex,]
lmRatings <- lm(Book.Rating ~ ., data = train_ratings)


```

